{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aabha\\nAabriti\\nAadya\\nAagnya\\nAagya\\nAahana\\nAaisha\\nAaishka\\nAaja\\nAakanchhya\\nAakreeti\\nAakritee\\nAakriti\\nAan'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load File\n",
    "#Change the address as appropriate\n",
    "nep_names = open(r'C:\\Users\\Mnsh\\Documents\\data\\nepali_names.txt', 'r').read()\n",
    "nep_names[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing\n",
    "nep_names = nep_names.lower()\n",
    "nep_names = nep_names.replace(\"prajapati-shanti\", \"\").replace(\"chaudhary-roshan\", \"\").replace(\"ac-sah\", \"\").replace(\"dutta-sanjib\",\"\")\n",
    "nep_names = nep_names.replace(\".\", \"\")\n",
    "nep_names = re.sub(\"[\\(].*?[\\)]\", \"\", nep_names)\n",
    "set(nep_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the length of longest name and numbers of character in it\n",
    "\n",
    "this_name = 0\n",
    "\n",
    "#Count chars in longest name\n",
    "longest = 0\n",
    "\n",
    "#current name\n",
    "x_name =[]\n",
    "prev_longest = 0\n",
    "for i in range(len(nep_names)):\n",
    "        \n",
    "    this_name += 1\n",
    "    x_name.append(nep_names[i])\n",
    "        \n",
    "    # is not alphabet\n",
    "    if not nep_names[i].isalpha():\n",
    "        \n",
    "        prev_longest = longest\n",
    "        \n",
    "        longest = max(this_name, longest)\n",
    "        if prev_longest < longest:\n",
    "            \n",
    "            #the name catches the longest name\n",
    "            the_name = x_name\n",
    "            prev_longest = longest\n",
    "        x_name =[]\n",
    "        \n",
    "        this_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['z', 'i', 'p', 'p', 'y'],\n",
       " ['k',\n",
       "  'e',\n",
       "  's',\n",
       "  'h',\n",
       "  'w',\n",
       "  'a',\n",
       "  's',\n",
       "  'w',\n",
       "  'a',\n",
       "  'r',\n",
       "  'a',\n",
       "  'n',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  'a',\n",
       "  '\\n'],\n",
       " 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_name, the_name, longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars incl end_line: 61798 and there are unique chars: 27\n"
     ]
    }
   ],
   "source": [
    "data_size = len(nep_names)\n",
    "chars = sorted(list(set(nep_names))) #make a list of unique chars and sort them\n",
    "vocab_size = len(chars)\n",
    "print(\"Total chars incl end_line: %d and there are unique chars: %d\" % ( data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use a dict to convert from char to index and vice versa\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}  #has to be resorted despite having a sorted list\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From name as sequence of chars --> to sequence of ints\n",
    "\n",
    "#The first one is with padding --> names shorter than selected sequence length will be padded\n",
    "def get_tensors(names_list, seq_len):\n",
    "    names_tensor = torch.zeros(len(names_list), seq_len)\n",
    "    \n",
    "    \n",
    "    for i in range(len(names_list)):\n",
    "        names_in_ints = ([char_to_ix[j] for j in names_list[i]])\n",
    "        #if len(names_in_ints) < 17:\n",
    "        k = seq_len - len(names_in_ints)\n",
    "        fill_0 = [0] * k\n",
    "        names_in_ints.extend(fill_0)\n",
    "        names_tensor[i,:] = torch.tensor(names_in_ints)\n",
    "        \n",
    "    input_tensor = torch.zeros(names_tensor.shape)\n",
    "    input_tensor[:, 1:] = names_tensor[:, :-1]\n",
    "    #input_tensor[:10, :]\n",
    "        \n",
    "    return(input_tensor, names_tensor)\n",
    "\n",
    "#Alternative version --> The whole names are treated as continuous sequence\n",
    "#0 ('\\n') --> works as a marker where one name ends and other begins\n",
    "#Apparently --> the next name not only starts after 0, but it is related to the one before\n",
    "def get_tensors_alt(names_list, seq_len):\n",
    "    #names_tensor = torch.zeros(len(names_list), 17)\n",
    "    combined_names_ints = []\n",
    "    whole_input_names = [0]\n",
    "    \n",
    "    for i in range(len(names_list)):\n",
    "        names_tensor = torch.zeros(len(names_list[i]), 1)\n",
    "        names_in_ints = ([char_to_ix[j] for j in names_list[i]])\n",
    "        names_in_ints.append(0)\n",
    "        combined_names_ints.extend(names_in_ints)\n",
    "    whole_input_names.extend(combined_names_ints[:-1])\n",
    "    num_rows = len(whole_input_names)//seq_len\n",
    "    remainder = len(whole_input_names) - num_rows * seq_len\n",
    "    inputs_new, targets_new = torch.tensor(whole_input_names[:-remainder]).view(-1, seq_len), torch.tensor(combined_names_ints[:-remainder]).view(-1, seq_len) \n",
    "    return(inputs_new, targets_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aabha\\naabr'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nep_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8323"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#str_list = list(filter(None, str_list)) \n",
    "\n",
    "#regroup in names removin \\n\n",
    "all_names = nep_names.split('\\n')\n",
    "\n",
    "#enmpy names were innterferring --> ''\n",
    "all_names = list(filter(None, all_names)) \n",
    "\n",
    "#sort such that order is maintained\n",
    "all_names.sort()\n",
    "\n",
    "len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0., 26.,  1.,  8.,  5.,  5., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  1., 11., 11.,  9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  5., 14.,  9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  5., 14.,  9., 19.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  5., 14.,  9., 19.,  8.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  5., 14.,  9., 20.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  5., 14.,  9., 20.,  8.,  1., 18.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  9.,  1., 21., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  9.,  7.,  1., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [ 0., 26.,  9., 16., 16., 25.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.]]),\n",
       " tensor([[26.,  1.,  8.,  5.,  5., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  1., 11., 11.,  9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  5., 14.,  9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  5., 14.,  9., 19.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  5., 14.,  9., 19.,  8.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  5., 14.,  9., 20.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  5., 14.,  9., 20.,  8.,  1., 18.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  9.,  1., 21., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  9.,  7.,  1., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.],\n",
       "         [26.,  9., 16., 16., 25.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1,t1 = get_tensors(all_names[-10:], 15)\n",
    "i1,t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\nzaheer\\n\\n\\n\\n\\n\\n\\n\\n', 'zaheer\\n\\n\\n\\n\\n\\n\\n\\n\\n')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([ix_to_char[k] for k in i1[0].tolist()]), ''.join([ix_to_char[k] for k in t1[0].tolist()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0, 26,  1,  8,  5,  5, 18,  0, 26,  1, 11, 11,  9,  0, 26],\n",
       "         [ 5, 14,  9,  0, 26,  5, 14,  9, 19,  8,  0, 26,  5, 14,  9],\n",
       "         [19,  8,  1,  0, 26,  5, 14,  9, 20,  8,  0, 26,  5, 14,  9],\n",
       "         [20,  8,  1, 18,  0, 26,  9,  1, 21, 12,  0, 26,  9,  7,  1]]),\n",
       " tensor([[26,  1,  8,  5,  5, 18,  0, 26,  1, 11, 11,  9,  0, 26,  5],\n",
       "         [14,  9,  0, 26,  5, 14,  9, 19,  8,  0, 26,  5, 14,  9, 19],\n",
       "         [ 8,  1,  0, 26,  5, 14,  9, 20,  8,  0, 26,  5, 14,  9, 20],\n",
       "         [ 8,  1, 18,  0, 26,  9,  1, 21, 12,  0, 26,  9,  7,  1, 18]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2,t2 = get_tensors_alt(all_names[-10:], 15)\n",
    "i2,t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\nzaheer\\nzakki\\nz', 'zaheer\\nzakki\\nze')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([ix_to_char[k] for k in i2[0].tolist()]), ''.join([ix_to_char[k] for k in t2[0].tolist()] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 26, 1, 8, 5, 5, 18, 0, 26, 1, 11, 11, 9, 0, 26]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b\n",
      "b c\n",
      "c d\n",
      "d e\n",
      "e f\n",
      "f g\n",
      "g h\n",
      "h i\n",
      "i j\n",
      "j k\n",
      "k l\n",
      "l m\n",
      "m n\n",
      "n o\n",
      "o p\n",
      "p q\n",
      "q r\n",
      "r s\n",
      "s t\n",
      "t u\n",
      "u v\n",
      "v w\n",
      "w y\n",
      "y z\n"
     ]
    }
   ],
   "source": [
    "#find all names starting from each character\n",
    "#There are no names starting from 'x'\n",
    "counter = 1\n",
    "num_names_each_char = []\n",
    "\n",
    "for i in range(1, len(all_names)):\n",
    "    if all_names[i-1][0] == all_names[i][0]:\n",
    "        counter += 1\n",
    "    else:\n",
    "        print(all_names[i-1][0], all_names[i][0])\n",
    "        num_names_each_char.append(counter)\n",
    "        counter = 1\n",
    "num_names_each_char.append(counter) #As the last char ('z) is not yet appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([957,\n",
       "  663,\n",
       "  163,\n",
       "  431,\n",
       "  75,\n",
       "  25,\n",
       "  168,\n",
       "  150,\n",
       "  86,\n",
       "  283,\n",
       "  381,\n",
       "  152,\n",
       "  442,\n",
       "  462,\n",
       "  44,\n",
       "  678,\n",
       "  2,\n",
       "  701,\n",
       "  1888,\n",
       "  210,\n",
       "  125,\n",
       "  69,\n",
       "  10,\n",
       "  148,\n",
       "  10],\n",
       " 10,\n",
       " tensor(8323),\n",
       " 8323)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_names_each_char, counter, torch.sum(torch.tensor(num_names_each_char)), len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):      \n",
    "    e_x= np.exp(x)\n",
    "    return (e_x/np.sum(e_x))\n",
    "softmax([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5 -5 -5 -5 -5 -5 -4 -3 -2 -1  0  1  2  3  4  5  5  5  5  5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,\n",
       "         3,   4,   5,   6,   7,   8,   9])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_one = np.arange(-10, 10)#.reshape(10, 2)\n",
    "next_one = np.arange(-3, 30)\n",
    "\n",
    "def clip_it(it):\n",
    "    \n",
    "    it = np.maximum(-5, it)# axis =0)\n",
    "    it = np.minimum(it, 5)# axis =0)\n",
    "    return it\n",
    "print(clip_it(some_one))\n",
    "some_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights_backup(h_size, vocab_size):\n",
    "    Wxh = np.random.randn(h_size, vocab_size)*0.001\n",
    "    Whh = np.random.randn(h_size, h_size)*0.001\n",
    "    Why = np.random.randn(vocab_size, h_size)*0.001\n",
    "    bh = np.zeros((h_size, 1))\n",
    "    by = np.zeros((vocab_size, 1))\n",
    "    return Wxh, Whh, Why, bh, by\n",
    "\n",
    "#WEIGHT INITIALIZATIO - https://github.com/pangolulu/rnn-from-scratch\n",
    "\n",
    "def init_weights(h_size, vocab_size):\n",
    "    #Wxh = torch.randn(h_size, vocab_size)*0.001\n",
    "    Wxh = torch.FloatTensor(h_size, vocab_size).uniform_(-np.sqrt(1. / vocab_size), np.sqrt(1. / vocab_size))\n",
    "    #Whh = torch.randn(h_size, h_size)*0.001\n",
    "    Whh = torch.FloatTensor(h_size, h_size).uniform_(-np.sqrt(1. / h_size), np.sqrt(1. / h_size))\n",
    "\n",
    "    #Why = torch.randn(vocab_size, h_size)*0.001\n",
    "    Why = torch.FloatTensor(vocab_size, h_size).uniform_(-np.sqrt(1. / h_size), np.sqrt(1. / h_size))\n",
    "\n",
    "    bh = torch.zeros((h_size, 1))\n",
    "    by = torch.zeros((vocab_size, 1))\n",
    "    return Wxh, Whh, Why, bh, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_size =128 #Number nodes in hidden state\n",
    "#vocab_size = 27\n",
    "\n",
    "Wxh, Whh, Why, bh, by = init_weights(h_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq, target_seq = get_tensors(all_names[:950], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.253104209899902\n",
      "2 8.845074653625488\n",
      "4 8.646018028259277\n",
      "6 8.44072437286377\n",
      "8 8.257573127746582\n",
      "10 8.103699684143066\n",
      "12 7.975302696228027\n",
      "14 7.866878032684326\n",
      "16 7.7739481925964355\n",
      "18 7.6932148933410645\n",
      "20 7.622260570526123\n",
      "22 7.559297561645508\n",
      "24 7.502967834472656\n",
      "26 7.4522175788879395\n",
      "28 7.406204700469971\n",
      "30 7.364236354827881\n",
      "32 7.325735092163086\n",
      "34 7.290223121643066\n",
      "36 7.2573018074035645\n",
      "38 7.2266316413879395\n",
      "40 7.197950839996338\n",
      "42 7.1710381507873535\n",
      "44 7.1457109451293945\n",
      "46 7.121829509735107\n",
      "48 7.099274635314941\n",
      "50 7.077957630157471\n",
      "52 7.057797431945801\n",
      "54 7.0387282371521\n",
      "56 7.020697116851807\n",
      "58 7.003650665283203\n",
      "60 6.987541675567627\n",
      "62 6.972328186035156\n",
      "64 6.957968235015869\n",
      "66 6.944422245025635\n",
      "68 6.931650638580322\n",
      "70 6.919619560241699\n",
      "72 6.908289432525635\n",
      "74 6.8976240158081055\n",
      "76 6.887590408325195\n",
      "78 6.878159523010254\n",
      "80 6.869293689727783\n",
      "82 6.8609700202941895\n",
      "84 6.853157997131348\n",
      "86 6.845827102661133\n",
      "88 6.838954925537109\n",
      "90 6.832515716552734\n",
      "92 6.826486110687256\n",
      "94 6.820838451385498\n",
      "96 6.81556510925293\n",
      "98 6.810639381408691\n"
     ]
    }
   ],
   "source": [
    "#Forward propagation\n",
    "#Since the inputs, cell states, outputs are necessary for backprop, we will save them in a dictionary\n",
    "#As first hidden state expects input previous hidden state as input, we initiate hs[-1] as 0 vector of same dimension\n",
    "#Inputs and cell states are taken forward as given in the equation\n",
    "#Loss depends upon the probability assigned to the target character\n",
    "#Sum losses over each step to compute loss for the whole sequence\n",
    "\n",
    "# mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "# mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "mWxh, mWhh, mWhy = torch.zeros_like(Wxh), torch.zeros_like(Whh), torch.zeros_like(Why)\n",
    "mbh, mby = torch.zeros_like(bh), torch.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "l_r = 0.01\n",
    "eps = 1e-6\n",
    "losses = []\n",
    "#lr = 0.001\n",
    "seq_len = input_seq.size()[1]\n",
    "loss = 0\n",
    "xs, hs, ps, ys, yt = {}, {}, {}, {}, {}\n",
    "hs[-1] = torch.zeros((h_size, 1))\n",
    "for k in range(100):\n",
    "    for m in range(input_seq.size()[0]):\n",
    "        cur_inp_seq = input_seq[m]\n",
    "        cur_tar_seq = target_seq[m].type(torch.LongTensor)\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "        #for i in range(10):\n",
    "            one_hot = torch.zeros(27, 1)\n",
    "            one_hot[(cur_inp_seq[i].type(torch.LongTensor))] = 1\n",
    "            #print(one_hot)\n",
    "            #print(xs[i])\n",
    "            xs[i] = one_hot\n",
    "            hs[i] = torch.tanh(torch.mm(Wxh, xs[i]) + torch.mm(Whh, hs[i-1]) + bh)\n",
    "            ys[i] = torch.mm(Why, hs[i]) + by\n",
    "            ps[i] = torch.exp(ys[i]) / torch.sum(torch.exp(ys[i]))\n",
    "            #target = torch.argmax(targets[i])\n",
    "            target = cur_tar_seq[i]\n",
    "            loss += -torch.log(ps[i][target, 0])\n",
    "\n",
    "\n",
    "            #Backward propagation\n",
    "        #To get derivatives, initialize matrices of same dimension, all 0s\n",
    "        dWhy = torch.zeros(Why.shape)\n",
    "        dby = torch.zeros(by.shape)\n",
    "        dWhh = torch.zeros(Whh.shape)\n",
    "        dbh = torch.zeros(bh.shape)\n",
    "        dWxh = torch.zeros(Wxh.shape)\n",
    "\n",
    "        #Start working in reverse order and collect derivative at each step\n",
    "        for i in range(seq_len)[::-1]:\n",
    "        #for i in range(10)[::-1]:\n",
    "            \n",
    "\n",
    "            #position of 1 in target vector (y), implies 100% probability of that character\n",
    "            #####y_idx = torch.argmax(targets[i])  #position of 1 in target vector (y), implies 100% probability of that character\n",
    "\n",
    "\n",
    "            y_idx = cur_tar_seq[i]\n",
    "            #select the probability at same position and subtract 1 (y_hat - y)    \n",
    "            ps[i][y_idx] -= 1 \n",
    "\n",
    "\n",
    "            #For Why and by derivative with respect to error is sum of derivatives at each step or step-error\n",
    "            dWhy_step_i=ps[i] @ (hs[i]).t()      #(y_hat - y) @ hs.T - for each step\n",
    "\n",
    "            dby = dby + ps[i]                    #ps[i] represents (y_hat - y) as computed above\n",
    "\n",
    "            dWhy = dWhy + dWhy_step_i            #sum over each step\n",
    "\n",
    "\n",
    "            #Computing delta for Whx, Whh and bh invloves computing step-error\n",
    "            #Followed by finding differential of parameters each step-error over the whole sequnce\n",
    "            #computing and updating delta_z helps to keep track of and sum up throught out the sequence\n",
    "\n",
    "            #delta with respect to a single step\n",
    "            delta_recent = (Why.t()@ ps[i]) * (1-hs[i]**2)\n",
    "\n",
    "            #taking all the way back to first step\n",
    "            for j in range(i+1)[::-1]:\n",
    "\n",
    "                dWhh_ij= delta_recent @ hs[j-1].t()\n",
    "                dbh = dbh + delta_recent\n",
    "\n",
    "                dWhh = dWhh + dWhh_ij\n",
    "\n",
    "                dWxh_ij= delta_recent @ xs[j].t()\n",
    "                dWxh = dWxh + dWxh_ij\n",
    "\n",
    "                #update delta_z such that it goes back one step all while computing the derivative with repect\n",
    "                #to same step-error\n",
    "                delta_recent = (Whh.t() @ delta_recent) * (1-hs[j-1]**2)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        mWhy += dWhy * dWhy\n",
    "        mWxh += dWxh * dWxh\n",
    "        mWhh += dWhh * dWhh\n",
    "        mbh += dbh * dbh\n",
    "        mby += dby * dby\n",
    "        \n",
    "        Why += - l_r * dWhy/np.sqrt(mWhy +eps)\n",
    "        Whh +=  - l_r * dWhh/np.sqrt(mWhh +eps)\n",
    "        Wxh +=  - l_r * dWxh/np.sqrt(mWxh +eps)\n",
    "        by +=  - l_r * dby/np.sqrt(mby +eps)\n",
    "        bh +=  - l_r * dbh/np.sqrt(mbh +eps)\n",
    "        \n",
    "        \n",
    "#         Why = Why - lr * dWhy\n",
    "#         Whh = Whh - lr * dWhh\n",
    "#         Wxh = Wxh - lr * dWxh\n",
    "#         by = by - lr *dby\n",
    "#         bh = bh - lr *dbh\n",
    "\n",
    "\n",
    "    if k % 2 ==0 :\n",
    "        print(k, loss.item())\n",
    "        losses.append(loss.item())\n",
    "    #adjust the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "def gen_names(Wxh, Whh, Why, bh, by):\n",
    "    new_names = []\n",
    "    cur_inp_char = torch.Tensor([0])\n",
    "    elements = [i for i in range(27)]\n",
    "    gen_name = []\n",
    "    for x in range(50):\n",
    "        gen_name = []\n",
    "        for p in range(seq_len):\n",
    "            one_hot = torch.zeros(27, 1)\n",
    "            #one_hot[(cur_inp_seq[p].type(torch.LongTensor))] = 1\n",
    "            one_hot[(cur_inp_char.type(torch.LongTensor))] = 1\n",
    "            #print(one_hot)\n",
    "            #print(xs[i])\n",
    "            xs[p] = one_hot\n",
    "            hs[p] = torch.tanh(torch.mm(Wxh, xs[p]) + torch.mm(Whh, hs[p-1]) + bh)\n",
    "            ys[p] = torch.mm(Why, hs[p]) + by\n",
    "            ps[p] = torch.exp(ys[p]) / torch.sum(torch.exp(ys[p]))\n",
    "\n",
    "            #cur_inp_char = torch.topk(ps[p], 1, dim = 0)[1]\n",
    "\n",
    "            w=np.array(list(ps[p]))\n",
    "            #print(choice(elements, p=w/sum(w)))\n",
    "            cur_inp_char = torch.Tensor([choice(elements, p=w/sum(w))])\n",
    "            #cur_inp_char = torch.Tensor([np.random.choice(list(torch.topk(ps[p], 2, dim = 0)[1]))])\n",
    "            #gen_name.append(cur_inp_char.item())\n",
    "            if cur_inp_char ==0:\n",
    "                break\n",
    "            gen_name.append(cur_inp_char.item())\n",
    "        new_names.append(''.join([ix_to_char[f] for f in gen_name]))\n",
    "    #print(new_names)\n",
    "    return(new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avasya',\n",
       " 'alex',\n",
       " 'ashwip',\n",
       " 'anushya',\n",
       " 'aishan',\n",
       " 'amakashees',\n",
       " 'angadres',\n",
       " 'aygab',\n",
       " 'aviyat',\n",
       " 'arjan',\n",
       " 'ayusha',\n",
       " 'arisher',\n",
       " 'anir',\n",
       " 'anushiy',\n",
       " 'avinakh',\n",
       " 'aradh',\n",
       " 'arutrna',\n",
       " 'arit',\n",
       " 'bmees',\n",
       " 'adilp',\n",
       " 'asmi',\n",
       " 'arbind',\n",
       " 'anudhmal',\n",
       " 'assh',\n",
       " 'apgaral',\n",
       " 'arjan',\n",
       " 'angesha',\n",
       " 'arna',\n",
       " 'asdeet',\n",
       " 'apca',\n",
       " 'amja',\n",
       " 'alna',\n",
       " 'adeya',\n",
       " 'asma',\n",
       " 'avisekh',\n",
       " 'anushra',\n",
       " 'amod',\n",
       " 'amikesh',\n",
       " 'ashukana',\n",
       " 'anuv',\n",
       " 'asmi',\n",
       " 'ashsit',\n",
       " 'aray',\n",
       " 'ajanna',\n",
       " 'asha',\n",
       " 'armal',\n",
       " 'ayushu',\n",
       " 'asam',\n",
       " 'awarit',\n",
       " '']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_names(Wxh, Whh, Why, bh, by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Forward propagation\n",
    "#Since the inputs, cell states, outputs are necessary for backprop, we will save them in a dictionary\n",
    "#As first hidden state expects input previous hidden state as input, we initiate hs[-1] as 0 vector of same dimension\n",
    "#Inputs and cell states are taken forward as given in the equation\n",
    "#Loss depends upon the probability assigned to the target character\n",
    "#Sum losses over each step to compute loss for the whole sequence\n",
    "\n",
    "# mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "# mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "def rnn_net(Wxh, Whh, Why, bh, by, input_seq, target_seq, epochs):\n",
    "    seq_len = input_seq.size()[1] #+1\n",
    "    #print(seq_len)\n",
    "    #losses = []\n",
    "    mWxh, mWhh, mWhy = torch.zeros_like(Wxh), torch.zeros_like(Whh), torch.zeros_like(Why)\n",
    "    mbh, mby = torch.zeros_like(bh), torch.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    lr = 0.001\n",
    "    #seq_len = input_tensor.size()[1]\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys, yt = {}, {}, {}, {}, {}\n",
    "    hs[-1] = torch.zeros((h_size, 1))\n",
    "    for k in range(epochs):\n",
    "        #hs[-1] = torch.zeros((h_size, 1))\n",
    "        for m in range(input_seq.size()[0]):\n",
    "            cur_inp_seq = input_seq[m]#.view(-1,1)\n",
    "            cur_tar_seq = target_seq[m].type(torch.LongTensor)\n",
    "            \n",
    "            loss = 0\n",
    "            for i in range(seq_len):\n",
    "            #for i in range(10):\n",
    "                one_hot = torch.zeros(27, 1)\n",
    "                \n",
    "                one_hot[(cur_inp_seq[i].type(torch.LongTensor))] = 1\n",
    "                \n",
    "                xs[i] = one_hot\n",
    "                hs[i] = torch.tanh(torch.mm(Wxh, xs[i]) + torch.mm(Whh, hs[i-1]) + bh)\n",
    "                ys[i] = torch.mm(Why, hs[i]) + by\n",
    "                ps[i] = torch.exp(ys[i]) / torch.sum(torch.exp(ys[i]))\n",
    "                #target = torch.argmax(targets[i])\n",
    "                target = cur_tar_seq[i]\n",
    "                loss += -torch.log(ps[i][target, 0])\n",
    "\n",
    "            ####hs[-1] = hs[i]\n",
    "            \n",
    "            #Backward propagation\n",
    "            #To get derivatives, initialize matrices of same dimension, all 0s\n",
    "            dWhy = torch.zeros(Why.shape)\n",
    "            dby = torch.zeros(by.shape)\n",
    "            dWhh = torch.zeros(Whh.shape)\n",
    "            dbh = torch.zeros(bh.shape)\n",
    "            dWxh = torch.zeros(Wxh.shape)\n",
    "\n",
    "            #Start working in reverse order and collect derivative at each step\n",
    "            for i in range(seq_len)[::-1]:\n",
    "            #for i in range(10)[::-1]:\n",
    "\n",
    "\n",
    "                #position of 1 in target vector (y), implies 100% probability of that character\n",
    "                #####y_idx = torch.argmax(targets[i])  #position of 1 in target vector (y), implies 100% probability of that character\n",
    "\n",
    "\n",
    "                y_idx = cur_tar_seq[i]\n",
    "                #select the probability at same position and subtract 1 (y_hat - y)    \n",
    "                ps[i][y_idx] -= 1 \n",
    "\n",
    "\n",
    "                #For Why and by derivative with respect to error is sum of derivatives at each step or step-error\n",
    "                dWhy_step_i=ps[i] @ (hs[i]).t()      #(y_hat - y) @ hs.T - for each step\n",
    "\n",
    "                dby = dby + ps[i]                    #ps[i] represents (y_hat - y) as computed above\n",
    "\n",
    "                dWhy = dWhy + dWhy_step_i            #sum over each step\n",
    "\n",
    "\n",
    "                #Computing delta for Whx, Whh and bh invloves computing step-error\n",
    "                #Followed by finding differential of parameters each step-error over the whole sequnce\n",
    "                #computing and updating delta_z helps to keep track of and sum up throught out the sequence\n",
    "\n",
    "                #delta with respect to a single step\n",
    "                delta_recent = (Why.t()@ ps[i]) * (1-hs[i]**2)\n",
    "\n",
    "                #taking all the way back to first step\n",
    "                for j in range(i+1)[::-1]:\n",
    "\n",
    "                    dWhh_ij= delta_recent @ hs[j-1].t()\n",
    "                    dbh = dbh + delta_recent\n",
    "\n",
    "                    dWhh = dWhh + dWhh_ij\n",
    "\n",
    "                    dWxh_ij= delta_recent @ xs[j].t()\n",
    "                    dWxh = dWxh + dWxh_ij\n",
    "\n",
    "                    #update delta_z such that it goes back one step all while computing the derivative with repect\n",
    "                    #to same step-error\n",
    "                    delta_recent = (Whh.t() @ delta_recent) * (1-hs[j-1]**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            mWhy += dWhy * dWhy\n",
    "            mWxh += dWxh * dWxh\n",
    "            mWhh += dWhh * dWhh\n",
    "            mbh += dbh * dbh\n",
    "            mby += dby * dby\n",
    "\n",
    "            Why += - l_r * dWhy/np.sqrt(mWhy +eps)\n",
    "            Whh +=  - l_r * dWhh/np.sqrt(mWhh +eps)\n",
    "            Wxh +=  - l_r * dWxh/np.sqrt(mWxh +eps)\n",
    "            by +=  - l_r * dby/np.sqrt(mby +eps)\n",
    "            bh +=  - l_r * dbh/np.sqrt(mbh +eps)\n",
    "\n",
    "\n",
    "    #         Why = Why - lr * dWhy\n",
    "    #         Whh = Whh - lr * dWhh\n",
    "    #         Wxh = Wxh - lr * dWxh\n",
    "    #         by = by - lr *dby\n",
    "    #         bh = bh - lr *dbh\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if k % 2 ==0 :\n",
    "            print(k, loss.item())\n",
    "    return (losses)\n",
    "            \n",
    "        #adjust the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.120089054107666\n",
      "2 2.6177220344543457\n",
      "4 2.3097617626190186\n",
      "6 2.222156047821045\n",
      "8 2.191105365753174\n",
      "10 2.1795356273651123\n",
      "12 2.176025629043579\n",
      "14 2.1761364936828613\n",
      "16 2.17785906791687\n",
      "18 2.1802215576171875\n",
      "20 2.1827383041381836\n",
      "22 2.1851656436920166\n",
      "24 2.187401533126831\n",
      "26 2.1894168853759766\n",
      "28 2.1912121772766113\n",
      "30 2.1928040981292725\n",
      "32 2.1942245960235596\n",
      "34 2.1954948902130127\n",
      "36 2.196655511856079\n",
      "38 2.1977169513702393\n",
      "40 2.198709487915039\n",
      "42 2.199643611907959\n",
      "44 2.2005410194396973\n",
      "46 2.201413869857788\n",
      "48 2.202275037765503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alio',\n",
       " 'aadarsh',\n",
       " 'aniu',\n",
       " 'amita',\n",
       " 'alina',\n",
       " 'ashetm',\n",
       " 'abink',\n",
       " 'abhiru',\n",
       " 'aneesh',\n",
       " 'avishek',\n",
       " 'aalek',\n",
       " 'apekshya',\n",
       " 'amlen',\n",
       " 'anmol',\n",
       " 'aeenadh',\n",
       " 'aamil',\n",
       " 'arujidr',\n",
       " 'ajin',\n",
       " 'alina',\n",
       " 'abheep',\n",
       " 'arida',\n",
       " 'asekh',\n",
       " 'aites',\n",
       " 'albina',\n",
       " 'aneela',\n",
       " 'avisekh',\n",
       " 'areenca',\n",
       " 'askula',\n",
       " 'anuraag',\n",
       " 'asbha',\n",
       " 'adhan',\n",
       " 'arbina',\n",
       " 'andrew',\n",
       " 'arbina',\n",
       " 'alina',\n",
       " 'anika',\n",
       " 'ashuka',\n",
       " 'anuruda',\n",
       " 'abhimar',\n",
       " 'anur',\n",
       " 'abhila',\n",
       " 'apshaa',\n",
       " 'aworakh',\n",
       " 'anteea',\n",
       " 'aditi',\n",
       " 'ashma',\n",
       " 'anyula',\n",
       " 'ayusha',\n",
       " 'aplish',\n",
       " 'ajendr']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh2, Whh2, Why2, bh2, by2 = init_weights(h_size, vocab_size)\n",
    "input_seq, target_seq = get_tensors_alt(all_names[:950], 12)\n",
    "losses2 = rnn_net(Wxh2, Whh2, Why2, bh2, by2, input_seq, target_seq, 50)\n",
    "gen_names(Wxh2, Whh2, Why2, bh2, by2) #loss 2.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anjeep, Anbooda, Abinish, Ashma, Avilesh, Ajilekh, Avineek, Areshagran, Ashriti, Akaran, Anjaar, Awachmi, Arja, Arina, Apeeth, Ankulan, Anjalee, Anudekran, Aashuraa, Adeenaaz, Abinash, Anubhag, Alooh, Avicekh, Alech, Aplesh, Askar, Anikabh, Aratumu, Amita, Asyab, Allee, Arbilae, Ashuta, Anishab, Akshee, Abhishwa, Ankesh, Arrjetta, Araji, Agrish, Avismar, Abadees, Andeekh, Anuton, Asidea, Arbha, Anmol, Anbusana, Aayjeshra'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gen_names_new = gen_names(Wxh2, Whh2, Why2, bh2, by2)\n",
    "', '.join(w.capitalize() for w in gen_names_new)#.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Araola, Aanim, Aalish, Aarjal, Amreth, Aisha, Adeeti, Angmi, Ambha, Ankeek, Ajay, Akhish, Arnang, Abhila, Aasha, Aranin, Akmit, Arunamr, Ayooh, Anumb, Arbina, Abiya, Amar, Avicesh, Ayushi, Anir, Adodra, Anidu, Awirish, Apeklacay, Anar, Aamish, Ajeet, Aaresh, Anurutpa, Anujila, Abekh, Apima, Anurabgne, Awadhkhstoi, Anureep, Arif, Aksha, Apec, Avichan, Anurudr, Arajiyu, Alone, Aakshana, Aajadr'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_names_new = gen_names(Wxh2, Whh2, Why2, bh2, by2)\n",
    "', '.join(w.capitalize() for w in gen_names_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
